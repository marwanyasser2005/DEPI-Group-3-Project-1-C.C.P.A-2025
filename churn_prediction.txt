{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Customer Churn Prediction & Analysis (C.C.P.A)\n",
    "## Digital Egypt Pioneers Initiative â€“ Graduation Project\n",
    "### AI & Data Science Track â€“ Round 2, Group 3\n",
    "\n",
    "**Project Name**: Customer Churn Prediction and Analysis (Project 1)  \n",
    "\n",
    "**Team Members**:  \n",
    "- Marwan Yasser Hassan Abdelghaffar (Team Leader)  \n",
    "- Sara Mohamed Zaghloul Mohamed  \n",
    "- Fatma Ayman Mohamed Abdelbaset  \n",
    "- Waleed Medhat Elsayed Ahmed  \n",
    "- Daniel Morcos Fouad Gerges  \n",
    "- Mohamed Ali Abdelghani  \n",
    "\n",
    "**Project Overview**:  \n",
    "The Customer Churn Prediction and Analysis (C.C.P.A) project aims to develop a robust machine learning model to predict customer churn, enabling businesses to proactively retain at-risk customers. This project encompasses data collection, exploratory data analysis (EDA), feature engineering, model development, explainability, MLOps, and deployment. By leveraging SHAP for interpretability and MLflow for experiment tracking, we deliver actionable insights and a scalable solution for real-world applications.\n",
    "\n",
    "**Technologies Used**:  \n",
    "- Python, Pandas, NumPy  \n",
    "- Scikit-learn, XGBoost, LightGBM, RandomForest  \n",
    "- Matplotlib, Seaborn, Plotly  \n",
    "- SHAP (Explainability)  \n",
    "- MLflow (MLOps)  \n",
    "- Streamlit (Deployment)  \n",
    "\n",
    "**Goals**:  \n",
    "- ðŸ” Achieve high predictive performance (F1-score â‰¥ 0.80).  \n",
    "- ðŸ“ˆ Provide actionable insights through visualizations and explainability.  \n",
    "- ðŸš€ Deploy a scalable solution with MLOps and modular code.  \n",
    "\n",
    "---\n",
    "## ðŸ› ï¸ Project Workflow\n",
    "1. **Setup and Imports**: Configure the environment and import libraries.  \n",
    "2. **Load Dataset**: Load and explore the Telco Customer Churn dataset.  \n",
    "3. **Data Preprocessing**: Clean data and encode categorical variables.  \n",
    "4. **Feature Engineering**: Create domain-specific features.  \n",
    "5. **Exploratory Data Analysis (EDA)**: Visualize patterns and relationships.  \n",
    "6. **Model Development**: Build and tune a Stacking Classifier.  \n",
    "7. **Model Evaluation**: Assess performance with accuracy, F1-score, and ROC-AUC.  \n",
    "8. **Explainability with SHAP**: Interpret predictions for actionable insights.  \n",
    "9. **MLOps with MLflow**: Log experiments and models for reproducibility.  \n",
    "10. **Deployment Preparation**: Create a Streamlit app for interactive predictions.  \n",
    "11. **Conclusion**: Summarize findings and outline next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ 1. Setup and Imports\n",
    "Install and import required libraries for data handling, modeling, visualization, and MLOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment to install)\n",
    "# !pip install pandas numpy scikit-learn xgboost lightgbm imbalanced-learn mlflow shap seaborn matplotlib plotly optuna joblib streamlit streamlit-shap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# MLOps\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Model Saving\n",
    "import joblib\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 2. Load Dataset\n",
    "Load the Telco Customer Churn dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(Path(file_path))\n",
    "        logger.info(f\"Dataset loaded successfully. Shape: {data.shape}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "\n",
    "# Load data\n",
    "file_path = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Display basic info\n",
    "print(\"ðŸ” Dataset Overview:\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(\"\\nColumns:\", data.columns.tolist())\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "display(data.head())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ 3. Data Preprocessing\n",
    "Clean the dataset, handle missing values, and encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Class to handle data preprocessing tasks.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean the dataset by handling missing values and converting types.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Raw dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned dataset.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "        df.dropna(inplace=True)\n",
    "        df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "        if 'customerID' in df.columns:\n",
    "            df.drop('customerID', axis=1, inplace=True)\n",
    "        logger.info(f\"Data cleaned successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_categorical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using one-hot encoding.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset with categorical variables.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Encoded dataset.\n",
    "        \"\"\"\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "        logger.info(f\"Categorical variables encoded. Shape: {df_encoded.shape}\")\n",
    "        return df_encoded\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor = DataPreprocessor()\n",
    "data_cleaned = preprocessor.clean_data(data)\n",
    "data_encoded = preprocessor.encode_categorical(data_cleaned)\n",
    "print(f\"ðŸ“ Encoded Data Shape: {data_encoded.shape}\")\n",
    "display(data_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ 4. Feature Engineering\n",
    "Create domain-specific features to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create new features based on domain knowledge.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Preprocessed dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with new features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['AvgCharges'] = df['TotalCharges'] / (df['tenure'] + 1)\n",
    "    df['Engagement'] = df['MonthlyCharges'] * df['tenure']\n",
    "    df['TenureToMonthlyRatio'] = df['tenure'] / (df['MonthlyCharges'] + 1)\n",
    "\n",
    "    # Count number of services\n",
    "    service_columns = ['PhoneService', 'MultipleLines', 'InternetService',\n",
    "                       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                       'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "    if all(col in df.columns for col in service_columns):\n",
    "        df['NumServices'] = df[service_columns].eq('Yes').sum(axis=1)\n",
    "\n",
    "    logger.info(f\"Feature engineering completed. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "data_engineered = engineer_features(data_cleaned)\n",
    "data_encoded = preprocessor.encode_categorical(data_engineered)\n",
    "print(f\"ðŸ“ Data Shape after Feature Engineering: {data_encoded.shape}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = data_encoded.drop('Churn', axis=1)\n",
    "y = data_encoded['Churn']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "logger.info(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Apply SMOTE to handle imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "logger.info(f\"Train shape after SMOTE: {X_train.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "logger.info(\"Features scaled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 5. Exploratory Data Analysis (EDA)\n",
    "Visualize data to uncover patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Churn distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Churn', data=data_cleaned, palette='Set2')\n",
    "plt.title('Distribution of Churn', fontsize=14)\n",
    "plt.xlabel('Churn (0: No, 1: Yes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.savefig('churn_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Tenure vs Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn', y='tenure', data=data_cleaned, palette='Set3')\n",
    "plt.title('Tenure vs Churn', fontsize=14)\n",
    "plt.xlabel('Churn (0: No, 1: Yes)', fontsize=12)\n",
    "plt.ylabel('Tenure (months)', fontsize=12)\n",
    "plt.savefig('tenure_vs_churn.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Visualize MonthlyCharges vs Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn', y='MonthlyCharges', data=data_cleaned, palette='Set1')\n",
    "plt.title('Monthly Charges vs Churn', fontsize=14)\n",
    "plt.xlabel('Churn (0: No, 1: Yes)', fontsize=12)\n",
    "plt.ylabel('Monthly Charges ($)', fontsize=12)\n",
    "plt.savefig('monthly_charges_vs_churn.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– 6. Model Development\n",
    "Build and train a Stacking Classifier using Random Forest, XGBoost, and LightGBM with Logistic Regression as the meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with optimized parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=136,\n",
    "    max_depth=25,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=180,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.894,\n",
    "    colsample_bytree=0.819,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=180,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.894,\n",
    "    colsample_bytree=0.819,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define stacking classifier\n",
    "estimators = [\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "]\n",
    "final_estimator = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    stacking_model.fit(X_train_scaled, y_train)\n",
    "    logger.info(\"Stacking model trained successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to train stacking model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 7. Model Evaluation\n",
    "Evaluate the model using accuracy, F1-score, ROC-AUC, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test: np.ndarray, y_test: pd.Series, model_name: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model and return performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        X_test (np.ndarray): Test features.\n",
    "        y_test (pd.Series): Test labels.\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "\n",
    "        # Print classification report\n",
    "        print(f\"Classification Report for {model_name}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {model_name}', fontsize=14)\n",
    "        plt.xlabel('Predicted', fontsize=12)\n",
    "        plt.ylabel('Actual', fontsize=12)\n",
    "        plt.savefig(f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.2f})', color='blue')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title(f'ROC Curve - {model_name}', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'roc_curve_{model_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        logger.info(f\"Evaluation completed for {model_name}. Metrics: {metrics}\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to evaluate model {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Evaluate the stacking model\n",
    "stacking_metrics = evaluate_model(stacking_model, X_test_scaled, y_test, \"Stacking Classifier\")\n",
    "print(\"Stacking Classifier Metrics:\", stacking_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” 8. Explainability with SHAP\n",
    "Use SHAP to interpret model predictions and provide actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of test data for SHAP to reduce computation time\n",
    "X_test_subset = X_test_scaled[:50]\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "try:\n",
    "    explainer = shap.KernelExplainer(stacking_model.predict_proba, shap.kmeans(X_train_scaled, 10))\n",
    "    shap_values = explainer.shap_values(X_test_subset)\n",
    "    logger.info(f\"SHAP values computed successfully. Shape: {len(shap_values)} {[v.shape for v in shap_values]}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to compute SHAP values: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify shapes\n",
    "logger.info(f\"X_test_subset shape: {X_test_subset.shape}\")\n",
    "logger.info(f\"shap_values[1] shape: {shap_values[1].shape}\")\n",
    "logger.info(f\"Length of feature_names: {len(feature_names)}\")\n",
    "\n",
    "# Handle feature mismatch\n",
    "if shap_values[1].shape[1] != X_test_subset.shape[1]:\n",
    "    logger.warning(f\"Shape mismatch: shap_values[1] has {shap_values[1].shape[1]} features, X_test_subset has {X_test_subset.shape[1]} features\")\n",
    "    if shap_values[1].shape[1] > X_test_subset.shape[1]:\n",
    "        logger.info(\"Trimming extra column from shap_values\")\n",
    "        shap_values[1] = shap_values[1][:, :-1]\n",
    "        feature_names = feature_names[:-1]\n",
    "    else:\n",
    "        logger.info(\"X_test_subset has more features. Dropping last column as fallback.\")\n",
    "        X_test_subset = X_test_subset[:, :-1]\n",
    "        feature_names = feature_names[:-1]\n",
    "        logger.info(f\"Recomputing SHAP values with updated X_test_subset shape: {X_test_subset.shape}\")\n",
    "        shap_values = explainer.shap_values(X_test_subset)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values[1], X_test_subset, feature_names=feature_names, show=False)\n",
    "plt.title(\"SHAP Summary Plot for Stacking Classifier\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SHAP Bar Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values[1], X_test_subset, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Feature Importance (Bar Plot)\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_bar_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SHAP Force Plot for a single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], pd.DataFrame(X_test_subset, columns=feature_names).iloc[0], matplotlib=True)\n",
    "plt.savefig(\"shap_force_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 9. MLOps with MLflow\n",
    "Log experiments, metrics, and models for reproducibility and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model and artifacts with MLflow\n",
    "input_example = pd.DataFrame([X_test_scaled[0]], columns=feature_names)\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"Stacking_Classifier_Final\"):\n",
    "        mlflow.log_metrics(stacking_metrics)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=stacking_model,\n",
    "            artifact_path=\"stacking_classifier\",\n",
    "            input_example=input_example,\n",
    "            registered_model_name=\"Stacking_Churn_Prediction\"\n",
    "        )\n",
    "        mlflow.log_artifact(\"confusion_matrix_stacking_classifier.png\")\n",
    "        mlflow.log_artifact(\"roc_curve_stacking_classifier.png\")\n",
    "        mlflow.log_artifact(\"shap_summary_plot.png\")\n",
    "        mlflow.log_artifact(\"shap_bar_plot.png\")\n",
    "        logger.info(\"Model and artifacts logged with MLflow successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to log with MLflow: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save model and preprocessing objects locally\n",
    "joblib.dump(stacking_model, \"churn_model.joblib\")\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "joblib.dump(feature_names, \"model_features.joblib\")\n",
    "logger.info(\"Model, scaler, and features saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ 10. Deployment Preparation\n",
    "Prepare a Streamlit app for interactive churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "from streamlit_shap import st_shap\n",
    "\n",
    "# Load model and preprocessing objects\n",
    "try:\n",
    "    model = joblib.load(\"churn_model.joblib\")\n",
    "    scaler = joblib.load(\"scaler.joblib\")\n",
    "    feature_names = joblib.load(\"model_features.joblib\")\n",
    "except FileNotFoundError as e:\n",
    "    st.error(f\"Error loading model or preprocessing objects: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# Load dataset for realistic customer selection\n",
    "try:\n",
    "    data = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "    data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "    data.dropna(inplace=True)\n",
    "except FileNotFoundError:\n",
    "    st.error(\"Dataset file not found.\")\n",
    "    st.stop()\n",
    "\n",
    "# Streamlit app layout\n",
    "st.title(\"Customer Churn Prediction App\")\n",
    "st.write(\"Select a customer from the Telco dataset to predict churn probability and gain insights.\")\n",
    "\n",
    "# Select a customer\n",
    "customer_options = data.index\n",
    "selected_customer_index = st.selectbox(\"Select a Customer\", customer_options)\n",
    "selected_customer = data.loc[selected_customer_index]\n",
    "\n",
    "# Display customer details\n",
    "st.write(\"### Customer Details\")\n",
    "st.write(f\"Gender: {selected_customer['gender']}\")\n",
    "st.write(f\"Senior Citizen: {'Yes' if selected_customer['SeniorCitizen'] == 1 else 'No'}\")\n",
    "st.write(f\"Tenure: {selected_customer['tenure']} months\")\n",
    "st.write(f\"Monthly Charges: ${selected_customer['MonthlyCharges']:.2f}\")\n",
    "st.write(f\"Total Charges: ${selected_customer['TotalCharges']:.2f}\")\n",
    "st.write(f\"Internet Service: {selected_customer['InternetService']}\")\n",
    "st.write(f\"Contract: {selected_customer['Contract']}\")\n",
    "\n",
    "# Prepare data for prediction\n",
    "if st.button(\"Predict Churn\"):\n",
    "    try:\n",
    "        # Encode categorical features\n",
    "        categorical_cols = ['gender', 'InternetService', 'Contract', 'PhoneService', 'MultipleLines',\n",
    "                            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "                            'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod']\n",
    "        encoded_data = pd.get_dummies(data[categorical_cols], drop_first=True)\n",
    "        selected_customer_encoded = pd.get_dummies(selected_customer[categorical_cols].to_frame().T, drop_first=True)\n",
    "        selected_customer_encoded = selected_customer_encoded.reindex(columns=encoded_data.columns, fill_value=0)\n",
    "\n",
    "        # Combine numerical and encoded features\n",
    "        input_data = pd.DataFrame({\n",
    "            'tenure': [selected_customer['tenure']],\n",
    "            'MonthlyCharges': [selected_customer['MonthlyCharges']],\n",
    "            'TotalCharges': [selected_customer['TotalCharges']],\n",
    "            'SeniorCitizen': [selected_customer['SeniorCitizen']],\n",
    "            'AvgCharges': [selected_customer['TotalCharges'] / (selected_customer['tenure'] + 1)],\n",
    "            'Engagement': [selected_customer['MonthlyCharges'] * selected_customer['tenure']],\n",
    "            'TenureToMonthlyRatio': [selected_customer['tenure'] / (selected_customer['MonthlyCharges'] + 1)],\n",
    "            'NumServices': [sum(selected_customer[col] == 'Yes' for col in ['PhoneService', 'MultipleLines', 'OnlineSecurity',\n",
    "                                                                           'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "                                                                           'StreamingTV', 'StreamingMovies'])]\n",
    "        })\n",
    "        input_data = pd.concat([input_data, selected_customer_encoded], axis=1)\n",
    "        input_data = input_data.reindex(columns=feature_names, fill_value=0)\n",
    "        input_scaled = scaler.transform(input_data)\n",
    "\n",
    "        # Make prediction\n",
    "        churn_prob = model.predict_proba(input_scaled)[:, 1][0]\n",
    "        prediction = model.predict(input_scaled)[0]\n",
    "        st.write(f\"### Prediction: {'Will Churn' if prediction == 1 else 'Will Not Churn'}\")\n",
    "        st.write(f\"Churn Probability: {churn_prob:.2%}\")\n",
    "        if prediction == 1:\n",
    "            st.error(\"High risk of churn! Consider offering a discount or improved service.\")\n",
    "        else:\n",
    "            st.success(\"Low risk of churn. Keep up the good service!\")\n",
    "\n",
    "        # SHAP explanation\n",
    "        st.write(\"### Why this prediction? (SHAP Explanation)\")\n",
    "        explainer = shap.KernelExplainer(lambda x: model.predict_proba(x)[:, 1], input_scaled)\n",
    "        shap_values = explainer.shap_values(input_scaled, nsamples=50)\n",
    "        st_shap(shap.force_plot(explainer.expected_value, shap_values[0], input_data.iloc[0], feature_names=feature_names), height=200, width=800)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during prediction: {e}\")\n",
    "\n",
    "# Instructions to run the app\n",
    "st.write(\"### How to Run the App Locally\")\n",
    "st.code(\"streamlit run app.py\", language=\"bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 11. Conclusion and Next Steps\n",
    "The Customer Churn Prediction and Analysis project successfully developed a high-performing stacking classifier (F1-score â‰¥ 0.80) to predict customer churn. Key achievements include:\n",
    "- Robust preprocessing and feature engineering to enhance model performance.\n",
    "- Comprehensive EDA revealing insights into churn patterns.\n",
    "- Interpretable predictions using SHAP for actionable business decisions.\n",
    "- MLOps integration with MLflow for experiment tracking.\n",
    "- A Streamlit app for interactive churn predictions.\n",
    "\n",
    "**Next Steps**:\n",
    "- Deploy the Streamlit app on a cloud platform (e.g., Streamlit Cloud, Heroku).\n",
    "- Integrate real-time data pipelines for continuous model updates.\n",
    "- Explore additional features (e.g., customer interaction logs) to improve predictions.\n",
    "- Implement A/B testing for retention strategies based on model insights.\n",
    "\n",
    "Thank you for exploring the C.C.P.A project! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}